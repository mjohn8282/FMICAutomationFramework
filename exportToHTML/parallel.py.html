<html>
<head>
<title>parallel.py</title>
<meta http-equiv="Content-Type" content="text/html; charset=utf-8">
<style type="text/css">
.s0 { color: #5c6370;}
.s1 { color: #abb2bf;}
.s2 { color: #c678dd;}
.s3 { color: #98c379;}
</style>
</head>
<body bgcolor="#282c34">
<table CELLSPACING=0 CELLPADDING=5 COLS=1 WIDTH="100%" BGCOLOR="#606060" >
<tr><td><center>
<font face="Arial, Helvetica" color="#000000">
parallel.py</font>
</center></td></tr></table>
<pre><span class="s0"># Licensed under the GPL: https://www.gnu.org/licenses/old-licenses/gpl-2.0.html</span>
<span class="s0"># For details: https://github.com/PyCQA/pylint/blob/master/LICENSE</span>

<span class="s2">import </span><span class="s1">collections</span>
<span class="s2">import </span><span class="s1">functools</span>

<span class="s2">from </span><span class="s1">pylint </span><span class="s2">import </span><span class="s1">reporters</span>
<span class="s2">from </span><span class="s1">pylint.lint.utils </span><span class="s2">import </span><span class="s1">_patch_sys_path</span>
<span class="s2">from </span><span class="s1">pylint.message </span><span class="s2">import </span><span class="s1">Message</span>

<span class="s2">try</span><span class="s1">:</span>
    <span class="s2">import </span><span class="s1">multiprocessing</span>
<span class="s2">except </span><span class="s1">ImportError:</span>
    <span class="s1">multiprocessing = </span><span class="s2">None  </span><span class="s0"># type: ignore</span>

<span class="s0"># PyLinter object used by worker processes when checking files using multiprocessing</span>
<span class="s0"># should only be used by the worker processes</span>
<span class="s1">_worker_linter = </span><span class="s2">None</span>


<span class="s2">def </span><span class="s1">_get_new_args(message):</span>
    <span class="s1">location = (</span>
        <span class="s1">message.abspath,</span>
        <span class="s1">message.path,</span>
        <span class="s1">message.module,</span>
        <span class="s1">message.obj,</span>
        <span class="s1">message.line,</span>
        <span class="s1">message.column,</span>
    <span class="s1">)</span>
    <span class="s2">return </span><span class="s1">(message.msg_id, message.symbol, location, message.msg, message.confidence)</span>


<span class="s2">def </span><span class="s1">_merge_stats(stats):</span>
    <span class="s1">merged = {}</span>
    <span class="s1">by_msg = collections.Counter()</span>
    <span class="s2">for </span><span class="s1">stat </span><span class="s2">in </span><span class="s1">stats:</span>
        <span class="s1">message_stats = stat.pop(</span><span class="s3">&quot;by_msg&quot;</span><span class="s1">, {})</span>
        <span class="s1">by_msg.update(message_stats)</span>

        <span class="s2">for </span><span class="s1">key, item </span><span class="s2">in </span><span class="s1">stat.items():</span>
            <span class="s2">if </span><span class="s1">key </span><span class="s2">not in </span><span class="s1">merged:</span>
                <span class="s1">merged[key] = item</span>
            <span class="s2">elif </span><span class="s1">isinstance(item, dict):</span>
                <span class="s1">merged[key].update(item)</span>
            <span class="s2">else</span><span class="s1">:</span>
                <span class="s1">merged[key] = merged[key] + item</span>

    <span class="s1">merged[</span><span class="s3">&quot;by_msg&quot;</span><span class="s1">] = by_msg</span>
    <span class="s2">return </span><span class="s1">merged</span>


<span class="s2">def </span><span class="s1">_worker_initialize(linter, arguments=</span><span class="s2">None</span><span class="s1">):</span>
    <span class="s2">global </span><span class="s1">_worker_linter  </span><span class="s0"># pylint: disable=global-statement</span>
    <span class="s1">_worker_linter = linter</span>

    <span class="s0"># On the worker process side the messages are just collected and passed back to</span>
    <span class="s0"># parent process as _worker_check_file function's return value</span>
    <span class="s1">_worker_linter.set_reporter(reporters.CollectingReporter())</span>
    <span class="s1">_worker_linter.open()</span>

    <span class="s0"># Patch sys.path so that each argument is importable just like in single job mode</span>
    <span class="s1">_patch_sys_path(arguments </span><span class="s2">or </span><span class="s1">())</span>


<span class="s2">def </span><span class="s1">_worker_check_single_file(file_item):</span>
    <span class="s1">name, filepath, modname = file_item</span>

    <span class="s1">_worker_linter.open()</span>
    <span class="s1">_worker_linter.check_single_file(name, filepath, modname)</span>
    <span class="s1">mapreduce_data = collections.defaultdict(list)</span>
    <span class="s2">for </span><span class="s1">checker </span><span class="s2">in </span><span class="s1">_worker_linter.get_checkers():</span>
        <span class="s2">try</span><span class="s1">:</span>
            <span class="s1">data = checker.get_map_data()</span>
        <span class="s2">except </span><span class="s1">AttributeError:</span>
            <span class="s2">continue</span>
        <span class="s1">mapreduce_data[checker.name].append(data)</span>
    <span class="s1">msgs = [_get_new_args(m) </span><span class="s2">for </span><span class="s1">m </span><span class="s2">in </span><span class="s1">_worker_linter.reporter.messages]</span>
    <span class="s1">_worker_linter.reporter.reset()</span>
    <span class="s2">return </span><span class="s1">(</span>
        <span class="s1">id(multiprocessing.current_process()),</span>
        <span class="s1">_worker_linter.current_name,</span>
        <span class="s1">filepath,</span>
        <span class="s1">_worker_linter.file_state.base_name,</span>
        <span class="s1">msgs,</span>
        <span class="s1">_worker_linter.stats,</span>
        <span class="s1">_worker_linter.msg_status,</span>
        <span class="s1">mapreduce_data,</span>
    <span class="s1">)</span>


<span class="s2">def </span><span class="s1">_merge_mapreduce_data(linter, all_mapreduce_data):</span>
    <span class="s0">&quot;&quot;&quot;Merges map/reduce data across workers, invoking relevant APIs on checkers&quot;&quot;&quot;</span>
    <span class="s0"># First collate the data, preparing it so we can send it to the checkers for</span>
    <span class="s0"># validation. The intent here is to collect all the mapreduce data for all checker-</span>
    <span class="s0"># runs across processes - that will then be passed to a static method on the</span>
    <span class="s0"># checkers to be reduced and further processed.</span>
    <span class="s1">collated_map_reduce_data = collections.defaultdict(list)</span>
    <span class="s2">for </span><span class="s1">linter_data </span><span class="s2">in </span><span class="s1">all_mapreduce_data.values():</span>
        <span class="s2">for </span><span class="s1">run_data </span><span class="s2">in </span><span class="s1">linter_data:</span>
            <span class="s2">for </span><span class="s1">checker_name, data </span><span class="s2">in </span><span class="s1">run_data.items():</span>
                <span class="s1">collated_map_reduce_data[checker_name].extend(data)</span>

    <span class="s0"># Send the data to checkers that support/require consolidated data</span>
    <span class="s1">original_checkers = linter.get_checkers()</span>
    <span class="s2">for </span><span class="s1">checker </span><span class="s2">in </span><span class="s1">original_checkers:</span>
        <span class="s2">if </span><span class="s1">checker.name </span><span class="s2">in </span><span class="s1">collated_map_reduce_data:</span>
            <span class="s0"># Assume that if the check has returned map/reduce data that it has the</span>
            <span class="s0"># reducer function</span>
            <span class="s1">checker.reduce_map_data(linter, collated_map_reduce_data[checker.name])</span>


<span class="s2">def </span><span class="s1">check_parallel(linter, jobs, files, arguments=</span><span class="s2">None</span><span class="s1">):</span>
    <span class="s0">&quot;&quot;&quot;Use the given linter to lint the files with given amount of workers (jobs) 
    This splits the work filestream-by-filestream. If you need to do work across 
    multiple files, as in the similarity-checker, then inherit from MapReduceMixin and 
    implement the map/reduce mixin functionality&quot;&quot;&quot;</span>
    <span class="s0"># The reporter does not need to be passed to worker processes, i.e. the reporter does</span>
    <span class="s1">original_reporter = linter.reporter</span>
    <span class="s1">linter.reporter = </span><span class="s2">None</span>

    <span class="s0"># The linter is inherited by all the pool's workers, i.e. the linter</span>
    <span class="s0"># is identical to the linter object here. This is required so that</span>
    <span class="s0"># a custom PyLinter object can be used.</span>
    <span class="s1">initializer = functools.partial(_worker_initialize, arguments=arguments)</span>
    <span class="s1">pool = multiprocessing.Pool(  </span><span class="s0"># pylint: disable=consider-using-with</span>
        <span class="s1">jobs, initializer=initializer, initargs=[linter]</span>
    <span class="s1">)</span>
    <span class="s0"># ..and now when the workers have inherited the linter, the actual reporter</span>
    <span class="s0"># can be set back here on the parent process so that results get stored into</span>
    <span class="s0"># correct reporter</span>
    <span class="s1">linter.set_reporter(original_reporter)</span>
    <span class="s1">linter.open()</span>
    <span class="s2">try</span><span class="s1">:</span>
        <span class="s1">all_stats = []</span>
        <span class="s1">all_mapreduce_data = collections.defaultdict(list)</span>

        <span class="s0"># Maps each file to be worked on by a single _worker_check_single_file() call,</span>
        <span class="s0"># collecting any map/reduce data by checker module so that we can 'reduce' it</span>
        <span class="s0"># later.</span>
        <span class="s2">for </span><span class="s1">(</span>
            <span class="s1">worker_idx,  </span><span class="s0"># used to merge map/reduce data across workers</span>
            <span class="s1">module,</span>
            <span class="s1">file_path,</span>
            <span class="s1">base_name,</span>
            <span class="s1">messages,</span>
            <span class="s1">stats,</span>
            <span class="s1">msg_status,</span>
            <span class="s1">mapreduce_data,</span>
        <span class="s1">) </span><span class="s2">in </span><span class="s1">pool.imap_unordered(_worker_check_single_file, files):</span>
            <span class="s1">linter.file_state.base_name = base_name</span>
            <span class="s1">linter.set_current_module(module, file_path)</span>
            <span class="s2">for </span><span class="s1">msg </span><span class="s2">in </span><span class="s1">messages:</span>
                <span class="s1">msg = Message(*msg)</span>
                <span class="s1">linter.reporter.handle_message(msg)</span>
            <span class="s1">all_stats.append(stats)</span>
            <span class="s1">all_mapreduce_data[worker_idx].append(mapreduce_data)</span>
            <span class="s1">linter.msg_status |= msg_status</span>
    <span class="s2">finally</span><span class="s1">:</span>
        <span class="s1">pool.close()</span>
        <span class="s1">pool.join()</span>

    <span class="s1">_merge_mapreduce_data(linter, all_mapreduce_data)</span>
    <span class="s1">linter.stats = _merge_stats(all_stats)</span>

    <span class="s0"># Insert stats data to local checkers.</span>
    <span class="s2">for </span><span class="s1">checker </span><span class="s2">in </span><span class="s1">linter.get_checkers():</span>
        <span class="s2">if </span><span class="s1">checker </span><span class="s2">is not </span><span class="s1">linter:</span>
            <span class="s1">checker.stats = linter.stats</span>
</pre>
</body>
</html>